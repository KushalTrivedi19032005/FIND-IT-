{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12624528,"sourceType":"datasetVersion","datasetId":7976641},{"sourceId":12624858,"sourceType":"datasetVersion","datasetId":7976908}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 1e-4\nk_folds=5\n\nCSV_PATH = \"/kaggle/input/ifnd-text/tokenized_updated (1).csv\"\nIMAGE_DIR = \"/kaggle/input/ifnd-images/resized_images\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:46:09.861043Z","iopub.execute_input":"2025-07-30T18:46:09.861814Z","iopub.status.idle":"2025-07-30T18:46:09.865459Z","shell.execute_reply.started":"2025-07-30T18:46:09.861790Z","shell.execute_reply":"2025-07-30T18:46:09.864800Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import os\nimport zipfile\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom transformers import DataCollatorWithPadding, DistilBertTokenizer\nfrom transformers.models.distilbert import DistilBertModel, DistilBertTokenizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom torch.optim.lr_scheduler import OneCycleLR\nimport numpy as np\nfrom sklearn.metrics import (\n    roc_auc_score, roc_curve,\n    accuracy_score, f1_score,\n    precision_score, recall_score,\n    precision_recall_curve, auc\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:46:12.219580Z","iopub.execute_input":"2025-07-30T18:46:12.219871Z","iopub.status.idle":"2025-07-30T18:46:12.225336Z","shell.execute_reply.started":"2025-07-30T18:46:12.219847Z","shell.execute_reply":"2025-07-30T18:46:12.224675Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"!pip install focal_loss_torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T00:44:55.449153Z","iopub.execute_input":"2025-07-24T00:44:55.449353Z","iopub.status.idle":"2025-07-24T00:44:58.533495Z","shell.execute_reply.started":"2025-07-24T00:44:55.449338Z","shell.execute_reply":"2025-07-24T00:44:58.532563Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: focal_loss_torch in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from focal_loss_torch) (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from focal_loss_torch) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->focal_loss_torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->focal_loss_torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->focal_loss_torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->focal_loss_torch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->focal_loss_torch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->focal_loss_torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->focal_loss_torch) (2024.2.0)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class NewsDataset(Dataset):\n    def __init__(self, csv_path, image_dir, transform=None,\n                 tokenizer_name='distilbert-base-uncased', max_length=64):\n        self.csv_path = csv_path\n        #self.image_zip_path = image_zip_path\n        self.image_dir = image_dir\n        self.transform = transform if transform else transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n        self.tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_name)\n        self.max_length = max_length\n\n        \n\n        # Extract images from zip if not already extracted\n        #if not os.path.exists(self.image_dir):\n         #   with zipfile.ZipFile(self.image_zip_path, 'r') as zip_ref:\n          #      zip_ref.extractall(self.image_dir)\n\n        # Load and clean the dataframe\n        self.df = pd.read_csv(self.csv_path, usecols=['id', 'Statement', 'Web', 'Category', 'Date', 'Label'])\n        # self.df.dropna(subset=['Statement'], inplace=True)\n\n        # # Use new scikit-learn API\n        # self.web_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n        # self.cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n        # # Fit and transform metadata\n        # self.web_encoded = self.web_encoder.fit_transform(self.df[['Web']])\n        # self.cat_encoded = self.cat_encoder.fit_transform(self.df[['Category']])\n\n        # # Assign unique column names to prevent index overlap\n        # web_df = pd.DataFrame(self.web_encoded, columns=[f'web_{i}' for i in range(self.web_encoded.shape[1])])\n        # cat_df = pd.DataFrame(self.cat_encoded, columns=[f'cat_{i}' for i in range(self.cat_encoded.shape[1])])\n\n        # # Concatenate metadata features\n        # meta_df = pd.concat([web_df, cat_df], axis=1)\n\n        # # Convert to torch tensor\n        # self.meta_features = torch.tensor(meta_df.values, dtype=torch.float32)\n\n        # self.meta_dim = self.meta_features.shape[1]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Text encoding\n        encoded = self.tokenizer(\n        row['Statement'],\n        truncation=True,\n        max_length=self.max_length,\n        return_attention_mask=True,\n        return_tensors=\"pt\"   # üî• Use tensors, let collator pad them later\n        )\n\n\n        # Image loading\n        img_path = os.path.join(self.image_dir, f\"img_{row['id']}.jpg\")\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image = self.transform(image)\n            image_mask = torch.tensor([1.0], dtype=torch.float32)  # Image is present\n        except FileNotFoundError:\n            image = torch.zeros(3, 224, 224)\n            image_mask = torch.tensor([0.0], dtype=torch.float32)  # Image is missing\n        #print(\"Image Mask Size in Dataset Class: \",image_mask.size())\n            \n\n        \n            \n            \n\n        return {\n            'input_ids': encoded['input_ids'].squeeze(0),\n            'attention_mask': encoded['attention_mask'].squeeze(0),\n            'image': image,\n            'label': int(row['Label']),\n            'id': int(row['id']),\n            'image_mask': image_mask\n            # 'meta': self.meta_features[idx]\n        }\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:46:26.434991Z","iopub.execute_input":"2025-07-30T18:46:26.435779Z","iopub.status.idle":"2025-07-30T18:46:26.449166Z","shell.execute_reply.started":"2025-07-30T18:46:26.435740Z","shell.execute_reply":"2025-07-30T18:46:26.448300Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        for param in self.model.parameters():\n            param.requires_grad = True\n        self.dropout = nn.Dropout(0.3);\n\n  #  def forward(self, input_ids, attention_mask):\n       # print(\">> DistilBERT received input_ids:\", input_ids[0, :10])\n   #     outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n       # print(\">> CLS embedding sample:\", outputs.last_hidden_state[:, 0, :5])\n    #    return outputs.last_hidden_state[:, 0, :]\n    def forward(self, input_ids, attention_mask):\n     outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n     hidden_states = outputs.last_hidden_state  # (batch, seq_len, 768)\n\n    # Mean pooling: sum(hidden * mask) / sum(mask)\n     input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n     sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n     sum_mask = input_mask_expanded.sum(1).clamp(min=1e-9)\n     return sum_embeddings / sum_mask\n\n\n# [BATCH, SEQ_LEN, HIDDEN_DIM] => [BATCH, HIDDEN_DIM]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:46:27.820673Z","iopub.execute_input":"2025-07-30T18:46:27.821402Z","iopub.status.idle":"2025-07-30T18:46:27.827222Z","shell.execute_reply.started":"2025-07-30T18:46:27.821372Z","shell.execute_reply":"2025-07-30T18:46:27.826392Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# class ResNetEncoder(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n#         self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n#         for param in self.feature_extractor.parameters():\n#             param.requires_grad = True\n\n\n#     def forward(self, x):\n#         x = self.feature_extractor(x).squeeze(-1).squeeze(-1)\n#         return x\n\n\nclass VGG19Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        backbone = models.vgg19(weights=models.VGG19_Weights.DEFAULT)\n        self.feature_extractor = backbone.features\n        self.avgpool = backbone.avgpool  # output is [batch_size, 512, 7, 7]\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(512 * 7 * 7, 2048)  # project to 2048 like ResNet50\n\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        x = self.avgpool(x)\n        x = self.flatten(x)       # shape: [batch_size, 25088]\n        x = self.fc(x)            # shape: [batch_size, 2048]\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:46:29.710495Z","iopub.execute_input":"2025-07-30T18:46:29.711077Z","iopub.status.idle":"2025-07-30T18:46:29.716809Z","shell.execute_reply.started":"2025-07-30T18:46:29.711050Z","shell.execute_reply":"2025-07-30T18:46:29.716005Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"\nclass SimpleAttentionFusion(nn.Module):\n    def __init__(self, text_dim=768, image_dim=2048, fusion_dim=512):\n        super(SimpleAttentionFusion, self).__init__()\n\n        # Project each modality to a common fusion dimension\n        self.text_proj = nn.Linear(text_dim, fusion_dim)\n        self.image_proj = nn.Linear(image_dim, fusion_dim)\n\n        # Attention score generator\n        self.attn = nn.Linear(fusion_dim, 1)\n\n    def forward(self, text_feat, image_feat,image_mask=None, return_attention=False):\n        # 1. Project to fusion_dim\n        text_feat = self.text_proj(text_feat)       # (B, 512)\n        image_feat = self.image_proj(image_feat)    # (B, 512)\n\n        # 2. L2 Normalize projected features\n        text_feat = F.normalize(text_feat, p=2, dim=1)   # (B, 512)\n        image_feat = F.normalize(image_feat, p=2, dim=1) # (B, 512)\n\n        # 3. Stack along modality dimension\n        x = torch.stack([text_feat, image_feat], dim=1)  # (B, 2, 512)\n\n        # 4. Compute attention logits\n        attn_logits = self.attn(x).squeeze(-1)  # (B, 2)\n        \n        # 5. If image is not present, give full weightage to text\n        # image_mask: (B,) ‚Üí (B, 1)\n        #print(\"Image Mask in Attention Fusion before Unsqueezing\", image_mask.size())\n        #image_mask = image_mask.unsqueeze(1)  # (B, 1)\n        # Determine if image modality is present (non-zero entries)\n        #print(\"Image Mask in Attention Fusion after Unsqueezing\", image_mask.size())     \n\n        # Stack to create modality mask\n        modality_mask = torch.stack([torch.ones_like(image_mask),image_mask], dim=1)  # Final shape: [B, 2]  # Shape: [B, 2]\n        #print(\"Modality mask\", modality_mask.size())\n        modality_mask= modality_mask.squeeze(1)\n        #print(\"Modality mask\", modality_mask.size())\n        #print(\"Attention Logits\", attn_logits.size())\n        # Mask out attention logits where modality is not present\n        attn_logits = attn_logits.masked_fill(modality_mask == 0, -1e9)  # Shape: [B, 2]\n\n        # 5. Sigmoid + renormalization\n        gates = torch.sigmoid(attn_logits)  # (B, 2) in (0, 1)\n        attn_weights = gates / gates.sum(dim=1, keepdim=True)  # (B, 2), sum to 1\n\n        # 6. Weighted sum to fuse\n        fused = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # (B, 512)\n\n        if return_attention:\n            return fused, attn_logits, attn_weights\n        return fused\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:46:31.715039Z","iopub.execute_input":"2025-07-30T18:46:31.715706Z","iopub.status.idle":"2025-07-30T18:46:31.725024Z","shell.execute_reply.started":"2025-07-30T18:46:31.715673Z","shell.execute_reply":"2025-07-30T18:46:31.724305Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class MLPReducer(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.classifier = nn.Sequential(\n           nn.Linear(input_dim, 256),\n           nn.ReLU(),\n           nn.Dropout(0.3),\n\n           nn.Linear(256, 128),\n           nn.ReLU(),\n           nn.Dropout(0.3),\n\n           nn.Linear(128, 64),\n           nn.ReLU(),\n           nn.Dropout(0.2),\n\n           nn.Linear(64, output_dim)  # Final binary output (logit)\n           )\n\n\n    def forward(self, x):\n        return self.classifier(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:04.041605Z","iopub.execute_input":"2025-07-30T18:47:04.042263Z","iopub.status.idle":"2025-07-30T18:47:04.046816Z","shell.execute_reply.started":"2025-07-30T18:47:04.042239Z","shell.execute_reply":"2025-07-30T18:47:04.046062Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom itertools import product\n\nclass ANFISClassifier(nn.Module):\n    def __init__(self, n_inputs, n_mfs):\n        super(ANFISClassifier, self).__init__()\n        self.n_inputs = n_inputs\n        self.n_mfs = n_mfs\n        self.n_rules = n_mfs ** n_inputs\n\n        # Gaussian MF parameters: centers (c) and sigmas for each MF\n        self.c = nn.Parameter(torch.linspace(-1, 1, steps=n_mfs).repeat(n_inputs, 1))     # (n_inputs, n_mfs)\n        self.sigma = nn.Parameter(torch.full((n_inputs, n_mfs), 0.5))\n        # Consequent layer: one linear function per rule (n_inputs + 1 for bias)\n        self.rule_params = self.rule_params = nn.Parameter(torch.zeros(self.n_rules, n_inputs + 1))\n        # Precompute MF index combinations for all rules\n        self.rule_indices = self._compute_rule_indices()\n\n    def _compute_rule_indices(self):\n        # Cartesian product of MF indices for each input feature (n_mfs^n_inputs combinations)\n        return torch.tensor(list(product(range(self.n_mfs), repeat=self.n_inputs)), dtype=torch.long)\n\n    def gaussian_mf(self, x, c, sigma):\n        # Gaussian Membership Function\n        return torch.exp(-((x - c)**2) / (2 * sigma**2 + 1e-6))\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        # Compute MF values: shape (batch, n_inputs, n_mfs)\n        mf_values = []\n        for i in range(self.n_inputs):\n            x_i = x[:, i].unsqueeze(1)                           # (batch, 1)\n            c_i = self.c[i].unsqueeze(0)                         # (1, n_mfs)\n            sigma_i = self.sigma[i].unsqueeze(0)                 # (1, n_mfs)\n            mf = self.gaussian_mf(x_i, c_i, sigma_i)             # (batch, n_mfs)\n            mf_values.append(mf)\n        mf_values = torch.stack(mf_values, dim=1)                # (batch, n_inputs, n_mfs)\n\n        # Compute firing strength of each rule: shape (batch, n_rules)\n        rule_strengths = []\n        for rule in self.rule_indices:\n            selected = mf_values[:, torch.arange(self.n_inputs), rule]  # (batch, n_inputs)\n            strength = torch.prod(selected, dim=1)                       # (batch,)\n            rule_strengths.append(strength)\n        w = torch.stack(rule_strengths, dim=1)                    # (batch, n_rules)\n\n        # Normalize firing strengths\n        normalized_w = w / (w.sum(dim=1, keepdim=True) + 1e-6)    # (batch, n_rules)\n\n        # Compute rule outputs: z_i = a1*x1 + a2*x2 + ... + an*xn + b\n        x_extended = torch.cat([x, torch.ones(batch_size, 1, device=x.device)], dim=1)  # (batch, n_inputs + 1)\n        z = torch.matmul(x_extended, self.rule_params.T)          # (batch, n_rules)\n\n        # Final output: weighted sum of rule outputs\n        output = (normalized_w * z).sum(dim=1)                    # (batch,)\n\n        return output.unsqueeze(1)  # Now returns shape (batch, 1)\n         # Binary classification output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:06.076700Z","iopub.execute_input":"2025-07-30T18:47:06.076971Z","iopub.status.idle":"2025-07-30T18:47:06.086780Z","shell.execute_reply.started":"2025-07-30T18:47:06.076950Z","shell.execute_reply":"2025-07-30T18:47:06.085949Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class FakeNewsDetectionModel(nn.Module):\n    def __init__(self):\n        super(FakeNewsDetectionModel, self).__init__()\n        self.text_encoder = TextEncoder()  # Outputs (batch_size, 768)\n        self.image_encoder = VGG19Encoder()  # Outputs (batch_size, 2048)\n\n        self.attn_fusion = SimpleAttentionFusion(\n            text_dim=768,\n            image_dim=2048,\n            # meta_dim=meta_dim,\n            fusion_dim=512\n        )  # Outputs (batch_size, 512)\n\n        self.reducer = MLPReducer(input_dim=512, output_dim=4)  # Outputs (batch_size, 4)\n        self.anfis = ANFISClassifier(n_inputs=4, n_mfs=2)       # Outputs (batch_size,)\n\n    def forward(self, input_ids, attention_mask, images,image_mask,return_image_feats=False, return_attention=False):\n     text_feats = self.text_encoder(input_ids, attention_mask)  # (batch, 768)\n     image_feats = self.image_encoder(images)# (batch, 2048)\n     #print(\"Image Mask Size Inside Model: \",image_mask.size())   \n\n     if return_attention:\n        fused_feats, attn_logits, attn_weights = self.attn_fusion(text_feats, image_feats,image_mask,return_attention=True)\n     else:\n        fused_feats = self.attn_fusion(text_feats, image_feats, image_mask)\n\n     reduced_feats = self.reducer(fused_feats)  # (batch, 4)\n     out = self.anfis(reduced_feats)            # (batch,)\n\n     if return_attention:\n        return (\n            out,            # final ANFIS output\n            image_feats,    # resnet\n            text_feats,     # distilbert\n            fused_feats,    # attention fusion output\n            reduced_feats,  # mlp reducer output\n            attn_logits,    # raw logits from attn layer\n            attn_weights    # normalized attention weights\n        )\n     elif return_image_feats:\n        return out, image_feats\n\n     return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:08.076635Z","iopub.execute_input":"2025-07-30T18:47:08.077170Z","iopub.status.idle":"2025-07-30T18:47:08.083263Z","shell.execute_reply.started":"2025-07-30T18:47:08.077146Z","shell.execute_reply":"2025-07-30T18:47:08.082619Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n     def __init__(self, alpha_bce=1.0, beta_focal=5.0, gamma_huber=0.5, pos_weight=None, delta=0.1):\n         super().__init__()\n         self.alpha = alpha_bce\n         self.beta = beta_focal\n         self.gamma = gamma_huber\n         self.pos_weight=pos_weight\n         self.delta = delta\n\n         self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n         self.huber = nn.HuberLoss(delta=delta)\n\n     def focal_loss(self, probs, targets, gamma=3):\n         eps = 1e-8\n         p_t = probs * targets + (1 - probs) * (1 - targets)\n         focal_term = (1 - p_t) ** gamma\n         return -torch.mean(focal_term * torch.log(p_t + eps))\n\n     def forward(self, logits, targets):\n         targets = targets.view(-1, 1).float()\n         bce_loss = self.bce(logits, targets)\n         probs = torch.sigmoid(logits)\n         focal_loss = self.focal_loss(probs, targets)\n         huber_loss = self.huber(probs, targets)\n         total_loss = self.alpha * bce_loss + self.beta * focal_loss + self.gamma * huber_loss\n         return total_loss, {\n             \"bce\": bce_loss.item(),\n             \"focal\": focal_loss.item(),\n             \"huber\": huber_loss.item()\n         }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:10.311506Z","iopub.execute_input":"2025-07-30T18:47:10.311818Z","iopub.status.idle":"2025-07-30T18:47:10.318261Z","shell.execute_reply.started":"2025-07-30T18:47:10.311797Z","shell.execute_reply":"2025-07-30T18:47:10.317511Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"train_loss_history = {\n    'bce': [[] for _ in range(k_folds)],\n    'focal': [[] for _ in range(k_folds)],\n    'huber': [[] for _ in range(k_folds)]\n}\n\nval_loss_history = {\n    'bce': [[] for _ in range(k_folds)],\n    'focal': [[] for _ in range(k_folds)],\n    'huber': [[] for _ in range(k_folds)]\n}\n\nval_accuracy_history = [[] for _ in range(k_folds)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:12.263739Z","iopub.execute_input":"2025-07-30T18:47:12.263988Z","iopub.status.idle":"2025-07-30T18:47:12.268584Z","shell.execute_reply.started":"2025-07-30T18:47:12.263967Z","shell.execute_reply":"2025-07-30T18:47:12.267917Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\nfrom torch.utils.data._utils.collate import default_collate\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nhf_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ndef custom_collate_fn(batch):\n    # Extract token-related fields and collate them using HuggingFace\n    token_keys = ['input_ids', 'attention_mask']\n    token_batch = {k: [d[k] for d in batch] for k in token_keys}\n    token_padded = hf_collator(token_batch)\n\n    # Manually collate the rest\n    labels = torch.tensor([d['label'] for d in batch])\n    images = torch.stack([d['image'] for d in batch])\n    ids = torch.tensor([d['id'] for d in batch])\n    image_masks = torch.tensor([item['image_mask'] for item in batch], dtype=torch.float32)\n    #print(\"Image Mask Size Returning from Collate Function :\",image_masks.size())\n\n    # Merge everything into one dictionary\n    return {\n        'input_ids': token_padded['input_ids'],\n        'attention_mask': token_padded['attention_mask'],\n        'label': labels,\n        'image': images,\n        'id': ids,\n        'image_mask': image_masks \n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:14.351504Z","iopub.execute_input":"2025-07-30T18:47:14.351797Z","iopub.status.idle":"2025-07-30T18:47:14.675187Z","shell.execute_reply.started":"2025-07-30T18:47:14.351775Z","shell.execute_reply":"2025-07-30T18:47:14.674643Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# === SETUP ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndataset = NewsDataset(csv_path=CSV_PATH, image_dir=IMAGE_DIR)\nlabels = [int(dataset.df.iloc[i]['Label']) for i in range(len(dataset))]\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\n\n\n\n# === EVALUATION ===\ndef evaluate(model, dataloader, criterion, device, debug_indices=None, dataset=None):\n    all_probs = []\n    all_preds = []\n    all_labels = []\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    loss_details = {\"bce\": 0.0, \"focal\": 0.0, \"huber\": 0.0}\n    TP = TN = FP = FN = 0\n\n    debug_outputs = []  # ‚¨Ö For storing debug sample results\n    anfis_inputs = {}   # ‚¨Ö New: store ANFIS input vectors\n\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['label'].to(device).float()\n            image_mask = batch['image_mask'].to(device)\n            # --- Forward pass with feature access ---\n            #text_feats = model.text_encoder(input_ids, attention_mask)  # (batch, 768)\n            #image_feats = model.image_encoder(images)                   # (batch, 2048)\n            #fused_feats = model.attn_fusion(text_feats, image_feats)   # (batch, 512)\n            #reduced_feats = model.reducer(fused_feats)                 # (batch, 4)\n            #outputs = model.anfis(reduced_feats)                       # (batch,)\n            # ‚¨áÔ∏è Run full forward pass with intermediate attention data\n            out = model(input_ids, attention_mask, images,image_mask=image_mask, return_image_feats=True, return_attention=True)\n            outputs, image_feats, text_feats, fused_feats, reduced_feats, attn_logits, attn_weights = out\n            #print(\"Outputs: \",outputs)\n            loss, details = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            for key in loss_details:\n                loss_details[key] += details[key]\n\n            probs = torch.sigmoid(outputs).squeeze()\n            preds = (probs > 0.5).long()\n            labels_long = labels.long()\n            all_probs.extend(probs.detach().cpu().numpy().tolist())\n            all_preds.extend(preds.detach().cpu().numpy().tolist())\n            all_labels.extend(labels_long.detach().cpu().numpy().tolist())\n            #print(\"Probabilites: \",probs)\n            correct += (preds == labels_long).sum().item()\n            total += labels.size(0)\n\n            TP += ((preds == 1) & (labels_long == 1)).sum().item()\n            TN += ((preds == 0) & (labels_long == 0)).sum().item()\n            FP += ((preds == 1) & (labels_long == 0)).sum().item()\n            FN += ((preds == 0) & (labels_long == 1)).sum().item()\n\n            # === DEBUG SAMPLE LOGGING ===\n            if debug_indices is not None and dataset is not None:\n                # If Subset, get the actual dataset indices\n                batch_indices = dataloader.dataset.indices if isinstance(dataloader.dataset, Subset) else range(len(dataset))\n                for j in range(len(labels)):\n                    global_idx = batch_indices[i * dataloader.batch_size + j]\n                    if global_idx in debug_indices:\n                        debug_outputs.append({\n                            'idx': global_idx,\n                            'prob': probs[j].item(),\n                            'pred': preds[j].item(),\n                            'true': labels[j].item()\n                        })#Logging these features for printing later\n                        anfis_inputs[global_idx] = {\n                            'reduced': reduced_feats[j].detach().cpu().numpy(),   # MLP Reducer output\n                            'imagevec': image_feats[j].detach().cpu().numpy(),    # RESNET output\n                            'textvec': text_feats[j].detach().cpu().numpy(),    # DistilBERT output\n                            'fused': fused_feats[j].detach().cpu().numpy(),# Attention Fusion output\n                            'attn_logits': attn_logits[j].detach().cpu().numpy(),\n                            'attn_weights': attn_weights[j].detach().cpu().numpy()\n                        }\n\n\n    accuracy = correct / total if total > 0 else 0.0\n    for key in loss_details:\n        loss_details[key] /= len(dataloader)\n\n    real_total = TP + FN\n    fake_total = TN + FP\n    confusion_stats = {\n        \"real_as_real_pct\": 100 * TP / real_total if real_total else 0.0,\n        \"real_as_fake_pct\": 100 * FN / real_total if real_total else 0.0,\n        \"fake_as_fake_pct\": 100 * TN / fake_total if fake_total else 0.0,\n        \"fake_as_real_pct\": 100 * FP / fake_total if fake_total else 0.0\n    }\n\n    return torch.tensor(all_labels),torch.tensor(all_probs),torch.tensor(all_preds),total_loss / len(dataloader), accuracy, loss_details, confusion_stats, debug_outputs, anfis_inputs\n\n\nall_fold_preds = []\nall_fold_labels = []\nall_fold_probs = []\n\n\n# === K-Fold Training Loop ===\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(np.zeros(len(dataset)), labels)):\n    print(f\"\\n=== Fold {fold+1}/{k_folds} ===\")\n    train_set = Subset(dataset, train_idx)\n    val_set = Subset(dataset, val_idx)\n\n   \n\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,collate_fn=custom_collate_fn)\n    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,collate_fn=custom_collate_fn)\n    \n    model = FakeNewsDetectionModel().to(device)\n    steps_per_epoch = len(train_loader)\n    epochs_per_fold = EPOCHS\n    num_folds = k_folds\n    total_steps = steps_per_epoch * epochs_per_fold * num_folds\n    pos_weight = torch.tensor([0.45], dtype=torch.float32).to(device)\n    criterion = CustomLoss(pos_weight=pos_weight)\n    #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    optimizer = torch.optim.AdamW([\n    {\"params\": model.text_encoder.parameters(), \"lr\": 1e-5},\n    {\"params\": model.image_encoder.parameters(), \"lr\": 3e-5},\n    {\"params\": model.attn_fusion.parameters(), \"lr\": 1e-4},\n    {\"params\": model.reducer.parameters(), \"lr\": 1e-3},\n    {\"params\": model.anfis.parameters(), \"lr\": 3e-4},], weight_decay=0.005)\n    scheduler = OneCycleLR(\n     optimizer,\n     max_lr=[1e-5, 3e-5, 1e-3, 1e-3, 1e-3],  # Match param groups\n     total_steps=total_steps,\n     pct_start=0.3,\n     anneal_strategy='cos',\n     div_factor=100.0,\n     final_div_factor=1e4)\n\n    #for name, param in model.named_parameters():\n     #if param.requires_grad:\n        #print(name)\n\n    # üîç Select 10 fixed validation indices for monitoring\n    val_indices_array = np.array(val_idx)\n    debug_indices = val_indices_array[np.random.choice(len(val_indices_array), size=10, replace=False)].tolist()\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss, bce_loss, focal_loss, huber_loss = 0.0, 0.0, 0.0, 0.0\n        batch_count = 0\n\n        for batch in tqdm(train_loader, desc=f\"Fold {fold+1}, Epoch {epoch+1}/{EPOCHS}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['label'].to(device).float()\n            image_mask = batch['image_mask'].to(device)\n\n            optimizer.zero_grad()\n            outputs, _ = model(input_ids, attention_mask, images, image_mask=image_mask,return_image_feats=True)\n            loss, details = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_loss += loss.item()\n            bce_loss += details['bce']\n            focal_loss += details['focal']\n            huber_loss += details['huber']\n            batch_count += 1\n            if batch_count % 300 == 0:\n              print(f\"[Batch {batch_count}] Loss: {loss.item():.4f} | BCE: {details['bce']:.4f} | \"\n               f\"Focal: {details['focal']:.4f} | Huber: {details['huber']:.4f}\")  # every 10 batches\n\n        labels,probs,preds,val_loss, val_acc, val_details, confusion_stats, debug_outputs, anfis_inputs = evaluate(model, val_loader, criterion, device, debug_indices=debug_indices, dataset=val_set)\n        if epoch == EPOCHS-1:\n                all_fold_preds.extend(preds.cpu().numpy().tolist())\n                all_fold_labels.extend(labels.cpu().numpy().tolist())\n                all_fold_probs.extend(probs.cpu().numpy().tolist())\n                #print(\"Extended Predictions: \",all_fold_preds)\n                #print(\"Extended Labels: \",all_fold_labels)\n                #print(\"Extended Probabilities: \",all_fold_probs)\n            \n                \n\n\n        # === Store per-fold, per-epoch values ===\n        train_loss_history['bce'][fold].append(bce_loss / batch_count)\n        train_loss_history['focal'][fold].append(focal_loss / batch_count)\n        train_loss_history['huber'][fold].append(huber_loss / batch_count)\n\n        val_loss_history['bce'][fold].append(val_details['bce'])\n        val_loss_history['focal'][fold].append(val_details['focal'])\n        val_loss_history['huber'][fold].append(val_details['huber'])\n\n        val_accuracy_history[fold].append(val_acc)\n\n        # Print Results\n        print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n        print(f\"  Train Loss: {epoch_loss/batch_count:.4f} | BCE: {bce_loss/batch_count:.4f} | \"\n              f\"Focal: {focal_loss/batch_count:.4f} | Huber: {huber_loss/batch_count:.4f}\")\n        print(f\"  Val Loss:   {val_loss:.4f} | Acc: {val_acc:.4f} | BCE: {val_details['bce']:.4f} | \"\n              f\"Focal: {val_details['focal']:.4f} | Huber: {val_details['huber']:.4f}\")\n        print(f\"  Real News Prediction:   {confusion_stats['real_as_real_pct']:.2f}% real, {confusion_stats['real_as_fake_pct']:.2f}% fake\")\n        print(f\"  Fake News Prediction:   {confusion_stats['fake_as_fake_pct']:.2f}% fake, {confusion_stats['fake_as_real_pct']:.2f}% real\")\n        # üîç Print sigmoid outputs + ANFIS inputs of 10 fixed samples\n        #print(\"\\n  [DEBUG] Sigmoid outputs and ANFIS inputs for 10 fixed validation samples:\")\n        \"\"\"for d in debug_outputs:\n          idx = d['idx']\n          print(f\"    Sample {idx}:\")\n          print(f\"      Prob = {d['prob']:.4f}, Pred = {d['pred']}, True = {int(d['true'])}\")\n          if idx in anfis_inputs:\n            reduced_str = \", \".join([f\"{v:.4f}\" for v in anfis_inputs[idx]['reduced']])\n            image_str = \", \".join([f\"{v:.4f}\" for v in anfis_inputs[idx]['imagevec']])\n            text_str = \", \".join([f\"{v:.4f}\" for v in anfis_inputs[idx]['textvec']])\n            fused_str = \", \".join([f\"{v:.4f}\" for v in anfis_inputs[idx]['fused']])\n            attlog_str = \", \".join([f\"{v:.4f}\" for v in anfis_inputs[idx]['attn_logits']])\n            attwt_str = \", \".join([f\"{v:.4f}\" for v in anfis_inputs[idx]['attn_weights']])  \n          # print(f\"      MLP Reducer Output = [{reduced_str}]\")    #Printing MLP Reducer Output\n            print(f\"      Attention Logits = [{attlog_str}]\")\n            print(f\"      Attention Weights = [{attwt_str}]\") \n            print(f\"      RESNET Output = [{image_str}]\")         #Printing Resnet Output\n            print(f\"      DistilBERT Output = [{text_str}]\") #Printing DistilBERT Output\n             \n            print(f\"      Attention Fusion Output = [{fused_str}]\")    #Printing Attention Fusion Output\n          else:\n            print(\"      ANFIS Input = Not Found ‚ùå\")\"\"\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T18:47:17.762975Z","iopub.execute_input":"2025-07-30T18:47:17.763256Z","iopub.status.idle":"2025-07-31T03:12:31.850978Z","shell.execute_reply.started":"2025-07-30T18:47:17.763233Z","shell.execute_reply":"2025-07-31T03:12:31.849974Z"}},"outputs":[{"name":"stdout","text":"\n=== Fold 1/5 ===\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  11%|‚ñà         | 300/2836 [02:15<18:32,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.9585 | BCE: 0.5029 | Focal: 0.0866 | Huber: 0.0450\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  21%|‚ñà‚ñà        | 600/2836 [04:28<16:34,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.9008 | BCE: 0.4537 | Focal: 0.0850 | Huber: 0.0448\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [06:40<14:19,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.7376 | BCE: 0.3631 | Focal: 0.0706 | Huber: 0.0427\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [08:52<11:31,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.9284 | BCE: 0.5341 | Focal: 0.0746 | Huber: 0.0430\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [11:04<09:48,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.7080 | BCE: 0.4001 | Focal: 0.0575 | Huber: 0.0404\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [13:16<07:44,  2.23it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.6020 | BCE: 0.3430 | Focal: 0.0480 | Huber: 0.0381\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [15:26<05:23,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.4976 | BCE: 0.2918 | Focal: 0.0376 | Huber: 0.0361\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [17:35<03:17,  2.21it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.3963 | BCE: 0.2365 | Focal: 0.0286 | Huber: 0.0337\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [19:46<00:58,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.5441 | BCE: 0.3064 | Focal: 0.0441 | Huber: 0.0344\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [20:47<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5]\n  Train Loss: 0.7012 | BCE: 0.3837 | Focal: 0.0595 | Huber: 0.0399\n  Val Loss:   0.4320 | Acc: 0.9650 | BCE: 0.2866 | Focal: 0.0260 | Huber: 0.0309\n  Real News Prediction:   98.24% real, 1.76% fake\n  Fake News Prediction:   93.02% fake, 6.98% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  11%|‚ñà         | 300/2836 [01:56<16:08,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.2942 | BCE: 0.2128 | Focal: 0.0136 | Huber: 0.0273\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  21%|‚ñà‚ñà        | 600/2836 [03:52<14:38,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.4426 | BCE: 0.2831 | Focal: 0.0290 | Huber: 0.0285\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:50<12:36,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.2800 | BCE: 0.2239 | Focal: 0.0089 | Huber: 0.0230\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:47<10:28,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.2753 | BCE: 0.2183 | Focal: 0.0090 | Huber: 0.0235\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:44<08:37,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.2396 | BCE: 0.1848 | Focal: 0.0090 | Huber: 0.0201\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:41<06:37,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.3111 | BCE: 0.2275 | Focal: 0.0146 | Huber: 0.0207\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:39<04:44,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.2749 | BCE: 0.1984 | Focal: 0.0135 | Huber: 0.0181\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:36<02:52,  2.53it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.1449 | BCE: 0.1300 | Focal: 0.0017 | Huber: 0.0128\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:34<00:53,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.1476 | BCE: 0.1281 | Focal: 0.0025 | Huber: 0.0143\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:28<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5]\n  Train Loss: 0.3060 | BCE: 0.2061 | Focal: 0.0178 | Huber: 0.0219\n  Val Loss:   0.1961 | Acc: 0.9728 | BCE: 0.1235 | Focal: 0.0133 | Huber: 0.0117\n  Real News Prediction:   98.49% real, 1.51% fake\n  Fake News Prediction:   94.87% fake, 5.13% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  11%|‚ñà         | 300/2836 [01:56<16:23,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.1078 | BCE: 0.1008 | Focal: 0.0006 | Huber: 0.0076\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  21%|‚ñà‚ñà        | 600/2836 [03:53<14:45,  2.53it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.1747 | BCE: 0.1152 | Focal: 0.0106 | Huber: 0.0125\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:49<12:39,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.1129 | BCE: 0.0943 | Focal: 0.0029 | Huber: 0.0086\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:46<10:35,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.2732 | BCE: 0.1118 | Focal: 0.0309 | Huber: 0.0136\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:44<08:29,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.1747 | BCE: 0.0833 | Focal: 0.0174 | Huber: 0.0086\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:42<06:36,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.0765 | BCE: 0.0692 | Focal: 0.0009 | Huber: 0.0055\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:39<04:44,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.2279 | BCE: 0.1283 | Focal: 0.0184 | Huber: 0.0153\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:36<02:53,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0715 | BCE: 0.0589 | Focal: 0.0019 | Huber: 0.0061\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:34<00:53,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.2014 | BCE: 0.1272 | Focal: 0.0138 | Huber: 0.0108\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:27<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5]\n  Train Loss: 0.1618 | BCE: 0.0980 | Focal: 0.0119 | Huber: 0.0089\n  Val Loss:   0.1350 | Acc: 0.9742 | BCE: 0.0668 | Focal: 0.0131 | Huber: 0.0054\n  Real News Prediction:   97.84% real, 2.16% fake\n  Fake News Prediction:   96.56% fake, 3.44% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  11%|‚ñà         | 300/2836 [01:56<16:34,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.1574 | BCE: 0.0961 | Focal: 0.0114 | Huber: 0.0088\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  21%|‚ñà‚ñà        | 600/2836 [03:52<14:04,  2.65it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.0439 | BCE: 0.0410 | Focal: 0.0003 | Huber: 0.0026\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:49<12:47,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.0888 | BCE: 0.0522 | Focal: 0.0068 | Huber: 0.0056\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:46<10:36,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.0681 | BCE: 0.0522 | Focal: 0.0029 | Huber: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:43<08:41,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.1700 | BCE: 0.0986 | Focal: 0.0135 | Huber: 0.0073\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:42<06:41,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.1940 | BCE: 0.0496 | Focal: 0.0284 | Huber: 0.0050\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:40<04:46,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0270 | BCE: 0.0238 | Focal: 0.0005 | Huber: 0.0020\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:37<02:50,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0210 | BCE: 0.0196 | Focal: 0.0001 | Huber: 0.0015\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:36<00:53,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.1561 | BCE: 0.0952 | Focal: 0.0113 | Huber: 0.0089\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:29<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/5]\n  Train Loss: 0.1189 | BCE: 0.0633 | Focal: 0.0105 | Huber: 0.0056\n  Val Loss:   0.1153 | Acc: 0.9761 | BCE: 0.0577 | Focal: 0.0110 | Huber: 0.0050\n  Real News Prediction:   99.15% real, 0.85% fake\n  Fake News Prediction:   94.53% fake, 5.47% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  11%|‚ñà         | 300/2836 [01:56<16:26,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.0166 | BCE: 0.0158 | Focal: 0.0000 | Huber: 0.0011\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  21%|‚ñà‚ñà        | 600/2836 [03:53<14:26,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.0485 | BCE: 0.0348 | Focal: 0.0024 | Huber: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:49<12:30,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.2859 | BCE: 0.0721 | Focal: 0.0419 | Huber: 0.0083\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:46<10:33,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.0432 | BCE: 0.0253 | Focal: 0.0032 | Huber: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:42<08:34,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.0504 | BCE: 0.0354 | Focal: 0.0026 | Huber: 0.0036\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:40<06:41,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.0793 | BCE: 0.0646 | Focal: 0.0024 | Huber: 0.0051\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:37<04:52,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0548 | BCE: 0.0430 | Focal: 0.0019 | Huber: 0.0049\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:34<02:46,  2.63it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0500 | BCE: 0.0363 | Focal: 0.0023 | Huber: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:31<00:52,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.1498 | BCE: 0.0732 | Focal: 0.0149 | Huber: 0.0046\n","output_type":"stream"},{"name":"stderr","text":"Fold 1, Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:24<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5]\n  Train Loss: 0.0987 | BCE: 0.0509 | Focal: 0.0091 | Huber: 0.0045\n  Val Loss:   0.1087 | Acc: 0.9785 | BCE: 0.0522 | Focal: 0.0108 | Huber: 0.0051\n  Real News Prediction:   98.60% real, 1.40% fake\n  Fake News Prediction:   96.35% fake, 3.65% real\n\n=== Fold 2/5 ===\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  11%|‚ñà         | 300/2836 [01:55<16:13,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.9342 | BCE: 0.4789 | Focal: 0.0865 | Huber: 0.0450\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  21%|‚ñà‚ñà        | 600/2836 [03:51<14:39,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.8481 | BCE: 0.4047 | Focal: 0.0842 | Huber: 0.0447\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:47<12:28,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.8715 | BCE: 0.4675 | Focal: 0.0764 | Huber: 0.0435\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:44<10:55,  2.50it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.6713 | BCE: 0.3467 | Focal: 0.0608 | Huber: 0.0411\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:40<08:45,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.6057 | BCE: 0.3273 | Focal: 0.0517 | Huber: 0.0394\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:37<06:36,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.6418 | BCE: 0.3723 | Focal: 0.0501 | Huber: 0.0384\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:34<04:48,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.5056 | BCE: 0.2975 | Focal: 0.0380 | Huber: 0.0361\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:31<02:46,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.6690 | BCE: 0.4264 | Focal: 0.0449 | Huber: 0.0360\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:29<00:53,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.4268 | BCE: 0.3142 | Focal: 0.0195 | Huber: 0.0303\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:22<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5]\n  Train Loss: 0.6943 | BCE: 0.3811 | Focal: 0.0587 | Huber: 0.0397\n  Val Loss:   0.4227 | Acc: 0.9618 | BCE: 0.2811 | Focal: 0.0253 | Huber: 0.0306\n  Real News Prediction:   96.63% real, 3.37% fake\n  Fake News Prediction:   95.29% fake, 4.71% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  11%|‚ñà         | 300/2836 [01:56<16:20,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.2867 | BCE: 0.2098 | Focal: 0.0127 | Huber: 0.0269\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  21%|‚ñà‚ñà        | 600/2836 [03:52<14:15,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.3160 | BCE: 0.2194 | Focal: 0.0167 | Huber: 0.0265\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:49<12:32,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.4431 | BCE: 0.2775 | Focal: 0.0306 | Huber: 0.0251\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:46<10:39,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.2995 | BCE: 0.2343 | Focal: 0.0108 | Huber: 0.0225\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:43<08:34,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.2884 | BCE: 0.2244 | Focal: 0.0107 | Huber: 0.0213\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:40<06:35,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.2556 | BCE: 0.1991 | Focal: 0.0094 | Huber: 0.0185\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:37<04:49,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.2053 | BCE: 0.1691 | Focal: 0.0054 | Huber: 0.0182\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:34<02:53,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.3133 | BCE: 0.2066 | Focal: 0.0194 | Huber: 0.0193\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:31<00:54,  2.50it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.1380 | BCE: 0.1070 | Focal: 0.0048 | Huber: 0.0143\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:24<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5]\n  Train Loss: 0.3062 | BCE: 0.2065 | Focal: 0.0178 | Huber: 0.0218\n  Val Loss:   0.2140 | Acc: 0.9660 | BCE: 0.1300 | Focal: 0.0155 | Huber: 0.0133\n  Real News Prediction:   96.55% real, 3.45% fake\n  Fake News Prediction:   96.70% fake, 3.30% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  11%|‚ñà         | 300/2836 [01:56<16:49,  2.51it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.3124 | BCE: 0.1728 | Focal: 0.0264 | Huber: 0.0149\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  21%|‚ñà‚ñà        | 600/2836 [03:53<14:35,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.2941 | BCE: 0.1420 | Focal: 0.0290 | Huber: 0.0140\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:49<12:48,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.3888 | BCE: 0.1454 | Focal: 0.0475 | Huber: 0.0116\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:46<10:38,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.1490 | BCE: 0.0942 | Focal: 0.0099 | Huber: 0.0103\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:44<08:52,  2.51it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.0742 | BCE: 0.0681 | Focal: 0.0006 | Huber: 0.0061\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:41<06:50,  2.53it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.1693 | BCE: 0.1274 | Focal: 0.0072 | Huber: 0.0123\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:38<04:49,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0470 | BCE: 0.0452 | Focal: 0.0001 | Huber: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:35<02:49,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0774 | BCE: 0.0679 | Focal: 0.0013 | Huber: 0.0065\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:33<00:54,  2.49it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.0371 | BCE: 0.0359 | Focal: 0.0000 | Huber: 0.0020\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:26<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5]\n  Train Loss: 0.1630 | BCE: 0.0991 | Focal: 0.0119 | Huber: 0.0089\n  Val Loss:   0.1497 | Acc: 0.9743 | BCE: 0.0942 | Focal: 0.0102 | Huber: 0.0090\n  Real News Prediction:   98.15% real, 1.85% fake\n  Fake News Prediction:   95.98% fake, 4.02% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  11%|‚ñà         | 300/2836 [01:55<16:18,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.0762 | BCE: 0.0687 | Focal: 0.0009 | Huber: 0.0061\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  21%|‚ñà‚ñà        | 600/2836 [03:51<14:51,  2.51it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.0311 | BCE: 0.0301 | Focal: 0.0001 | Huber: 0.0013\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:47<12:26,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.0536 | BCE: 0.0453 | Focal: 0.0012 | Huber: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:43<10:30,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.8604 | BCE: 0.1757 | Focal: 0.1357 | Huber: 0.0128\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:39<08:23,  2.65it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.0413 | BCE: 0.0382 | Focal: 0.0003 | Huber: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:36<06:38,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.0574 | BCE: 0.0367 | Focal: 0.0037 | Huber: 0.0041\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:34<04:49,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0999 | BCE: 0.0724 | Focal: 0.0050 | Huber: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:33<02:51,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0470 | BCE: 0.0369 | Focal: 0.0016 | Huber: 0.0043\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:31<00:52,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.0652 | BCE: 0.0418 | Focal: 0.0042 | Huber: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:24<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/5]\n  Train Loss: 0.1157 | BCE: 0.0630 | Focal: 0.0100 | Huber: 0.0054\n  Val Loss:   0.1179 | Acc: 0.9767 | BCE: 0.0514 | Focal: 0.0129 | Huber: 0.0043\n  Real News Prediction:   98.74% real, 1.26% fake\n  Fake News Prediction:   95.53% fake, 4.47% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  11%|‚ñà         | 300/2836 [01:56<16:22,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.8184 | BCE: 0.1689 | Focal: 0.1293 | Huber: 0.0056\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  21%|‚ñà‚ñà        | 600/2836 [03:53<15:13,  2.45it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.0343 | BCE: 0.0203 | Focal: 0.0026 | Huber: 0.0024\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:50<12:57,  2.49it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.0300 | BCE: 0.0202 | Focal: 0.0017 | Huber: 0.0023\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:46<10:37,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.0673 | BCE: 0.0552 | Focal: 0.0019 | Huber: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:43<08:42,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.0299 | BCE: 0.0277 | Focal: 0.0002 | Huber: 0.0025\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:39<06:41,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.1615 | BCE: 0.0998 | Focal: 0.0114 | Huber: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:37<04:46,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0029 | BCE: 0.0029 | Focal: 0.0000 | Huber: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:34<02:47,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0036 | BCE: 0.0035 | Focal: 0.0000 | Huber: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:31<00:53,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.0073 | BCE: 0.0071 | Focal: 0.0000 | Huber: 0.0003\n","output_type":"stream"},{"name":"stderr","text":"Fold 2, Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:24<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5]\n  Train Loss: 0.0965 | BCE: 0.0503 | Focal: 0.0088 | Huber: 0.0045\n  Val Loss:   0.1931 | Acc: 0.9745 | BCE: 0.0653 | Focal: 0.0252 | Huber: 0.0035\n  Real News Prediction:   99.58% real, 0.42% fake\n  Fake News Prediction:   93.21% fake, 6.79% real\n\n=== Fold 3/5 ===\n","output_type":"stream"},{"name":"stderr","text":"Fold 3, Epoch 1/5:  11%|‚ñà         | 300/2836 [01:56<15:53,  2.66it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.8612 | BCE: 0.4071 | Focal: 0.0863 | Huber: 0.0450\n","output_type":"stream"},{"name":"stderr","text":"Fold 3, Epoch 1/5:  21%|‚ñà‚ñà        | 600/2836 [03:51<14:23,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.9048 | BCE: 0.4543 | Focal: 0.0856 | Huber: 0.0449\n","output_type":"stream"},{"name":"stderr","text":"Fold 3, Epoch 1/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:47<12:33,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.7213 | BCE: 0.3419 | Focal: 0.0716 | Huber: 0.0428\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:52<11:06,  2.45it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.1851 | BCE: 0.1263 | Focal: 0.0108 | Huber: 0.0098\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:50<08:48,  2.53it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.8957 | BCE: 0.2604 | Focal: 0.1258 | Huber: 0.0121\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:48<06:45,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.1024 | BCE: 0.0720 | Focal: 0.0053 | Huber: 0.0076\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:46<04:54,  2.50it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0812 | BCE: 0.0575 | Focal: 0.0042 | Huber: 0.0057\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:45<02:50,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.1607 | BCE: 0.1021 | Focal: 0.0109 | Huber: 0.0080\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:44<00:54,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.0573 | BCE: 0.0543 | Focal: 0.0002 | Huber: 0.0042\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:37<00:00,  2.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5]\n  Train Loss: 0.1637 | BCE: 0.0989 | Focal: 0.0121 | Huber: 0.0089\n  Val Loss:   0.1595 | Acc: 0.9732 | BCE: 0.0702 | Focal: 0.0174 | Huber: 0.0045\n  Real News Prediction:   99.52% real, 0.48% fake\n  Fake News Prediction:   92.91% fake, 7.09% real\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  11%|‚ñà         | 300/2836 [01:57<16:42,  2.53it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 300] Loss: 0.1061 | BCE: 0.0869 | Focal: 0.0032 | Huber: 0.0065\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  21%|‚ñà‚ñà        | 600/2836 [03:54<14:52,  2.51it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 600] Loss: 0.1472 | BCE: 0.0588 | Focal: 0.0171 | Huber: 0.0052\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:51<12:29,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.0601 | BCE: 0.0483 | Focal: 0.0018 | Huber: 0.0052\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:49<10:49,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.0244 | BCE: 0.0238 | Focal: 0.0000 | Huber: 0.0011\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:47<08:47,  2.53it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.0729 | BCE: 0.0501 | Focal: 0.0040 | Huber: 0.0055\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:45<06:40,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.0796 | BCE: 0.0593 | Focal: 0.0033 | Huber: 0.0077\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:42<04:48,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0950 | BCE: 0.0777 | Focal: 0.0028 | Huber: 0.0067\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 4/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:40<02:50,  2.55it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0167 | BCE: 0.0160 | Focal: 0.0000 | Huber: 0.0012\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2836 [05:49<12:44,  2.53it/s]]","output_type":"stream"},{"name":"stdout","text":"[Batch 900] Loss: 0.2182 | BCE: 0.1274 | Focal: 0.0171 | Huber: 0.0107\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2836 [07:47<10:52,  2.51it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1200] Loss: 0.0198 | BCE: 0.0181 | Focal: 0.0002 | Huber: 0.0016\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2836 [09:44<08:29,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1500] Loss: 0.0462 | BCE: 0.0307 | Focal: 0.0027 | Huber: 0.0040\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2836 [11:42<06:50,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 1800] Loss: 0.1137 | BCE: 0.0658 | Focal: 0.0090 | Huber: 0.0058\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2100/2836 [13:40<04:59,  2.45it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2100] Loss: 0.0091 | BCE: 0.0090 | Focal: 0.0000 | Huber: 0.0003\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2836 [15:38<02:56,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2400] Loss: 0.0508 | BCE: 0.0299 | Focal: 0.0038 | Huber: 0.0041\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2700/2836 [17:36<00:52,  2.58it/s]","output_type":"stream"},{"name":"stdout","text":"[Batch 2700] Loss: 0.0260 | BCE: 0.0206 | Focal: 0.0008 | Huber: 0.0028\n","output_type":"stream"},{"name":"stderr","text":"Fold 5, Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2836/2836 [18:29<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5]\n  Train Loss: 0.0994 | BCE: 0.0514 | Focal: 0.0091 | Huber: 0.0045\n  Val Loss:   0.1288 | Acc: 0.9762 | BCE: 0.0662 | Focal: 0.0119 | Huber: 0.0067\n  Real News Prediction:   98.70% real, 1.30% fake\n  Fake News Prediction:   95.45% fake, 4.55% real\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# Convert to numpy arrays (safe)\ntrue_labels = np.array(all_fold_labels)\npredicted_probs = np.array(all_fold_probs)\npredicted_labels = np.array(all_fold_preds)\n\n# === Confusion Matrix ===\ncm = confusion_matrix(true_labels, predicted_labels)\ntn, fp, fn, tp = cm.ravel()\n# === Compute Metrics ===\naccuracy = accuracy_score(true_labels, predicted_labels)\nf1 = f1_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels)\nrecall = recall_score(true_labels, predicted_labels)\nroc_auc = roc_auc_score(true_labels, predicted_probs)\n\n# === ROC Curve ===\nfpr, tpr, _ = roc_curve(true_labels, predicted_probs)\nplt.figure(figsize=(6, 5))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', color='darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"roc_curve.png\")  # save for report\nplt.close()\n\n# === Precision-Recall Curve ===\nprecision_vals, recall_vals, _ = precision_recall_curve(true_labels, predicted_probs)\npr_auc = auc(recall_vals, precision_vals)\nplt.figure(figsize=(6, 5))\nplt.plot(recall_vals, precision_vals, label=f'PR Curve (AUC = {pr_auc:.4f})', color='purple')\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend(loc=\"upper right\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"pr_curve.png\")\nplt.close()\n\n# === Print Metrics ===\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"ROC AUC:   {roc_auc:.4f}\")\nprint(f\"PR AUC:    {pr_auc:.4f}\")\nprint(\"\\n=== Confusion Matrix ===\")\nprint(cm)\nprint(f\"True Negatives (TN): {tn}\")\nprint(f\"False Positives (FP): {fp}\")\nprint(f\"False Negatives (FN): {fn}\")\nprint(f\"True Positives (TP): {tp}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-24T01:57:46.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_kfold_losses(train_loss_history, val_loss_history, val_accuracy_history, epochs, k_folds):\n    # --- LOSS PLOT ---\n    plt.figure(figsize=(10, 6))\n    for fold in range(k_folds):\n        plt.plot(range(1, epochs + 1), train_loss_history['bce'][fold], label=f'Train BCE Fold {fold+1}', linestyle='-')\n        plt.plot(range(1, epochs + 1), val_loss_history['bce'][fold], label=f'Val BCE Fold {fold+1}', linestyle='--')\n\n        plt.plot(range(1, epochs + 1), train_loss_history['focal'][fold], label=f'Train Focal Fold {fold+1}', linestyle='-')\n        plt.plot(range(1, epochs + 1), val_loss_history['focal'][fold], label=f'Val Focal Fold {fold+1}', linestyle='--')\n\n        plt.plot(range(1, epochs + 1), train_loss_history['huber'][fold], label=f'Train Huber Fold {fold+1}', linestyle='-')\n        plt.plot(range(1, epochs + 1), val_loss_history['huber'][fold], label=f'Val Huber Fold {fold+1}', linestyle='--')\n\n    plt.title(\"Loss History Across Folds\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=7, loc='upper right', ncol=2)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"loss_plot.png\")  # Save loss plot\n    plt.close()\n\n    # --- ACCURACY PLOT ---\n    plt.figure(figsize=(10, 6))\n    for fold in range(k_folds):\n        plt.plot(range(1, epochs + 1), val_accuracy_history[fold], label=f'Val Acc Fold {fold+1}', marker='o')\n\n    plt.title(\"Validation Accuracy Across Folds\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(fontsize=8)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"accuracy_plot.png\")  # Save accuracy plot\n    plt.close()\n\n\nplot_kfold_losses(train_loss_history, val_loss_history, val_accuracy_history, EPOCHS, k_folds=5)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-24T01:57:46.389Z"}},"outputs":[],"execution_count":null}]}